{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "37476513c7f54e9f98a3979dccf77a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d9438c16037497f929b32275ca26b59",
              "IPY_MODEL_4ee301d78f9249d5bcca110c856931d7",
              "IPY_MODEL_ab5d981d8a734636b3c6550bc4480c02"
            ],
            "layout": "IPY_MODEL_4f94a40875a045a2a16854fc68474c9c"
          }
        },
        "8d9438c16037497f929b32275ca26b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e83f839bd6764f8c8d6dee31f68201f4",
            "placeholder": "​",
            "style": "IPY_MODEL_a708f9012a8345648846edc09c2ed56a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4ee301d78f9249d5bcca110c856931d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_142ab2c03c384cfb82907c1962e6eebe",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1fb158297f54aa793ce583b0509ef8f",
            "value": 3
          }
        },
        "ab5d981d8a734636b3c6550bc4480c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_223fe3e1bd7c4a8fb89da54584db4cdf",
            "placeholder": "​",
            "style": "IPY_MODEL_d0563fbda2a74f0ba2cef104f4dc060c",
            "value": " 3/3 [01:08&lt;00:00, 22.60s/it]"
          }
        },
        "4f94a40875a045a2a16854fc68474c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e83f839bd6764f8c8d6dee31f68201f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a708f9012a8345648846edc09c2ed56a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "142ab2c03c384cfb82907c1962e6eebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1fb158297f54aa793ce583b0509ef8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "223fe3e1bd7c4a8fb89da54584db4cdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0563fbda2a74f0ba2cef104f4dc060c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "904b2bf72b254b0d9ff42196967e7d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3527c4a2525c46818026f172b42ac722",
              "IPY_MODEL_210f7c0b1977494e86f38ee74b140969",
              "IPY_MODEL_86a5c8e2a67f4a6aa3c67dc07604ea74"
            ],
            "layout": "IPY_MODEL_dda7450eafa04fe19021587ee24427ac"
          }
        },
        "3527c4a2525c46818026f172b42ac722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d64753a7e654210806d022883c4cb15",
            "placeholder": "​",
            "style": "IPY_MODEL_7c9df362bfaf41db99ee25220b5bd8fe",
            "value": "Epoch 5, Loss: 0.2350: 100%"
          }
        },
        "210f7c0b1977494e86f38ee74b140969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8c59f10d71a4175880bf9bfe21b2e74",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19beb63148c443fca20126bd404209f3",
            "value": 25
          }
        },
        "86a5c8e2a67f4a6aa3c67dc07604ea74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b90c79a049e04174813194b513f11440",
            "placeholder": "​",
            "style": "IPY_MODEL_941abf1e2bb945c787a78acc98e65cb5",
            "value": " 25/25 [02:33&lt;00:00,  3.81s/it]"
          }
        },
        "dda7450eafa04fe19021587ee24427ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d64753a7e654210806d022883c4cb15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c9df362bfaf41db99ee25220b5bd8fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8c59f10d71a4175880bf9bfe21b2e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19beb63148c443fca20126bd404209f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b90c79a049e04174813194b513f11440": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "941abf1e2bb945c787a78acc98e65cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 Fine-Tuning a Multimodal Reward Model\n",
        "\n",
        "**Project Goal:** To build, train, and test a multimodal reward model capable of understanding and scoring textual descriptions of webpage screenshots.\n",
        "\n",
        "This notebook documents the entire end-to-end process, showcasing a real-world workflow for advanced AI alignment. We will start with raw data collection, create a preference dataset, fine-tune a powerful vision-language model using advanced memory-saving techniques, and finally, test the resulting model.\n",
        "\n",
        "**Why is this important?**\n",
        "A reward model is a critical component for aligning powerful AI systems with human preferences. It's the cornerstone of techniques like Reinforcement Learning from Human Feedback (RLHF) and is essential for developing capable web-automation agents that can understand visual interfaces.\n",
        "\n",
        "**Technology Stack:**\n",
        "* **Model:** LLaVA 1.5 (7B parameters)\n",
        "* **Framework:** PyTorch\n",
        "* **Key Libraries:** Hugging Face `transformers`, `accelerate`, and `bitsandbytes`\n",
        "* **Platform:** Google Colab (A100 GPU recommended)"
      ],
      "metadata": {
        "id": "8rrAJNa3bbaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr4l3B4I8j1G",
        "outputId": "51d534b8-5e8e-49ca-f085-8e67b621fcc4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Data Collection - Capturing Webpage Screenshots\n",
        "\n",
        "Every machine learning project begins with data. Our goal is to train a model that understands webpages, so our data will be screenshots of live websites.\n",
        "\n",
        "This cell sets up the environment to programmatically capture these screenshots.\n",
        "\n",
        "**What this code does:**\n",
        "1.  **Installs Dependencies:** It uses `pip` to install `selenium` and `webdriver-manager`, which are powerful tools for browser automation.\n",
        "2.  **Automates Chrome:** It controls a headless (invisible) Chrome browser directly within our Colab notebook.\n",
        "3.  **Captures Screenshots:** It iterates through a predefined list of `URLS`, navigates to each page, and saves a full-height PNG screenshot to a designated folder in Google Drive.\n",
        "4.  **Logs Metadata:** It creates a `captured_images.jsonl` file to keep a record of which image corresponds to which URL."
      ],
      "metadata": {
        "id": "tx-reXO3bUoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CHROME INSTALLATION TO BYPASS SYSTEM PACKAGES\n",
        "# =============================================================================\n",
        "\n",
        "# Step 1: Download and Install the official Google Chrome browser\n",
        "print(\"Downloading official Google Chrome browser...\")\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "print(\"Installing Google Chrome...\")\n",
        "# The -y flag automatically answers 'yes' to prompts\n",
        "!apt-get install -y ./google-chrome-stable_current_amd64.deb\n",
        "\n",
        "# Step 2: Install Selenium and the driver manager\n",
        "!pip install selenium webdriver-manager\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "import json\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# Step 3: Define the setup function pointing to our new Chrome installation\n",
        "def setup_driver():\n",
        "    \"\"\"Sets up the Selenium WebDriver using a manually installed Chrome binary.\"\"\"\n",
        "    options = webdriver.ChromeOptions()\n",
        "\n",
        "    # --- THIS IS THE CRITICAL NEW LINE ---\n",
        "    # Point Selenium to the binary of our manually installed Chrome\n",
        "    options.binary_location = \"/opt/google/chrome/google-chrome\"\n",
        "    # -------------------------------------\n",
        "\n",
        "    # Add all the stability flags from before\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    options.add_argument('--window-size=1920,1080')\n",
        "\n",
        "    print(\"Setting up Chrome driver to use the manually installed Google Chrome...\")\n",
        "    # webdriver-manager will now detect the version of OUR chrome and get the right driver\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=options)\n",
        "    print(\"Driver setup complete.\")\n",
        "    return driver\n",
        "\n",
        "# Step 4: Run the screenshot capture\n",
        "# Ensure Google Drive is mounted\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True) # force_remount after new installs\n",
        "\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/MultimodalRewardModel\"\n",
        "IMAGE_DIR = f\"{PROJECT_DIR}/images\"\n",
        "DATA_DIR = f\"{PROJECT_DIR}/data\"\n",
        "\n",
        "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "URLS = [\n",
        "   'https://www.wikipedia.org/',\n",
        "   'https://github.com/trending',\n",
        "   'https://www.allrecipes.com/search?q=pancakes',\n",
        "   'https://www.nytimes.com/section/technology',\n",
        "   'https://www.the-numbers.com/weekend-box-office-chart'\n",
        "]\n",
        "\n",
        "# Use our new setup function\n",
        "driver = setup_driver()\n",
        "image_records = []\n",
        "\n",
        "for url in URLS:\n",
        "    try:\n",
        "        print(f\"Navigating to {url}...\")\n",
        "        driver.get(url)\n",
        "        driver.set_window_size(1280, 800)\n",
        "        time.sleep(2)\n",
        "\n",
        "        image_filename = f\"{IMAGE_DIR}/{uuid.uuid4().hex}.png\"\n",
        "        driver.save_screenshot(image_filename)\n",
        "\n",
        "        if os.path.exists(image_filename) and os.path.getsize(image_filename) > 0:\n",
        "            image_records.append({\"url\": url, \"image_file\": image_filename})\n",
        "            print(f\"Successfully captured {url}\")\n",
        "        else:\n",
        "            print(f\"Failed to create a valid screenshot for {url}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error capturing {url}: {e}\")\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "# Save records\n",
        "with open(f\"{DATA_DIR}/captured_images.jsonl\", \"w\") as f:\n",
        "    for record in image_records:\n",
        "        f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "print(\"\\n✅ Screenshot capture should now be completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiN2RWdX_MYn",
        "outputId": "8a269b31-6cfc-4b4c-f8a8-c376a84a08e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading official Google Chrome browser...\n",
            "--2025-06-17 22:24:05--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 74.125.200.93, 74.125.200.190, 74.125.200.91, ...\n",
            "Connecting to dl.google.com (dl.google.com)|74.125.200.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117745852 (112M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 112.29M   408MB/s    in 0.3s    \n",
            "\n",
            "2025-06-17 22:24:05 (408 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [117745852/117745852]\n",
            "\n",
            "Installing Google Chrome...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'google-chrome-stable' instead of './google-chrome-stable_current_amd64.deb'\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  google-chrome-stable libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 10.9 MB/129 MB of archives.\n",
            "After this operation, 439 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 /content/google-chrome-stable_current_amd64.deb google-chrome-stable amd64 137.0.7151.119-1 [118 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.3 [10.7 MB]\n",
            "Fetched 10.9 MB in 3s (3,985 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 126319 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "Preparing to unpack .../google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (137.0.7151.119-1) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up google-chrome-stable (137.0.7151.119-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Collecting typing_extensions~=4.13.2 (from selenium)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, python-dotenv, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed outcome-1.3.0.post0 python-dotenv-1.1.0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 webdriver-manager-4.0.2 wsproto-1.2.0\n",
            "Mounted at /content/drive/\n",
            "Setting up Chrome driver to use the manually installed Google Chrome...\n",
            "Driver setup complete.\n",
            "Navigating to https://www.wikipedia.org/...\n",
            "Successfully captured https://www.wikipedia.org/\n",
            "Navigating to https://github.com/trending...\n",
            "Successfully captured https://github.com/trending\n",
            "Navigating to https://www.allrecipes.com/search?q=pancakes...\n",
            "Successfully captured https://www.allrecipes.com/search?q=pancakes\n",
            "Navigating to https://www.nytimes.com/section/technology...\n",
            "Successfully captured https://www.nytimes.com/section/technology\n",
            "Navigating to https://www.the-numbers.com/weekend-box-office-chart...\n",
            "Successfully captured https://www.the-numbers.com/weekend-box-office-chart\n",
            "\n",
            "✅ Screenshot capture should now be completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Creating a Preference Dataset\n",
        "\n",
        "A standard Large Language Model (LLM) knows what a webpage *is*, but it doesn't know what a *good description* of a webpage looks like from a human perspective. We align the model with our preferences by creating a special dataset.\n",
        "\n",
        "This cell builds a **preference dataset**, which is the foundation for training our reward model.\n",
        "\n",
        "**What this code does:**\n",
        "1.  **Loads Image Records:** It reads the `captured_images.jsonl` file created in the previous step.\n",
        "2.  **Defines Preference Pairs:** For each screenshot, we manually write:\n",
        "    * A **`prompt`**: The question we want to ask the AI (e.g., \"What is this webpage showing?\").\n",
        "    * A **`chosen`** response: A high-quality, accurate answer that we want the model to prefer.\n",
        "    * A **`rejected`** response: A plausible but incorrect or less helpful answer that we want the model to dis-prefer.\n",
        "3.  **Saves the Dataset:** It saves this data into a `train_preference.jsonl` file. Each line in this file contains a single preference pair (`image_path`, `prompt`, `chosen`, `rejected`), which is the exact format our training loop requires."
      ],
      "metadata": {
        "id": "7H3E7U1ibua9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# The 'image_records' variable should still be in your environment\n",
        "# from the previous step. If not, load it from the file.\n",
        "try:\n",
        "    print(f\"Using {len(image_records)} captured images.\")\n",
        "except NameError:\n",
        "    print(\"Loading image records from file...\")\n",
        "    DATA_DIR = \"/content/drive/MyDrive/MultimodalRewardModel/data\"\n",
        "    with open(f\"{DATA_DIR}/captured_images.jsonl\", \"r\") as f:\n",
        "        image_records = [json.loads(line) for line in f]\n",
        "\n",
        "# Manually create higher-quality preference pairs for our captured images.\n",
        "# This mimics what you would automate with an LLM for a larger dataset.\n",
        "preference_data = [\n",
        "    {\n",
        "        \"image\": image_records[0]['image_file'], # Wikipedia\n",
        "        \"prompt\": \"What is the primary purpose of this webpage?\",\n",
        "        \"chosen\": \"This is the homepage for Wikipedia, an online encyclopedia, allowing users to search for information across many languages.\",\n",
        "        \"rejected\": \"This is a shopping website for books.\"\n",
        "    },\n",
        "    {\n",
        "        \"image\": image_records[1]['image_file'], # GitHub Trending\n",
        "        \"prompt\": \"List some of the programming languages visible in the trending repositories section.\",\n",
        "        \"chosen\": \"The page shows trending repositories for languages like Python, TypeScript, and Rust.\",\n",
        "        \"rejected\": \"The only trending languages are Java and C++.\" # Plausible but incorrect for the specific screenshot\n",
        "    },\n",
        "    {\n",
        "        \"image\": image_records[2]['image_file'], # Allrecipes\n",
        "        \"prompt\": \"What kind of recipes are being displayed on this page?\",\n",
        "        \"chosen\": \"This page displays several recipes for pancakes, including 'Good Old-Fashioned Pancakes' and 'Fluffy Pancakes'.\",\n",
        "        \"rejected\": \"This page is about how to bake a chocolate cake.\"\n",
        "    },\n",
        "    {\n",
        "        \"image\": image_records[3]['image_file'], # NYT Technology\n",
        "        \"prompt\": \"Summarize the main headline visible in the screenshot.\",\n",
        "        \"chosen\": \"The main headline appears to be about a recent development or issue in the field of artificial intelligence.\",\n",
        "        \"rejected\": \"The main story is about a recent sports championship.\"\n",
        "    },\n",
        "    {\n",
        "        \"image\": image_records[4]['image_file'], # The Numbers\n",
        "        \"prompt\": \"According to this list, what is the top grosser movie of Weekend Domestic Chart for June 13, 2025?\",\n",
        "        \"chosen\": \"The list is topped by 'How to Train Your Dragon' with the Highest Gross revenue.\",\n",
        "        \"rejected\": \"The top-rated movie is 'Avatar'.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save the high-quality preference dataset to your Drive\n",
        "DATA_DIR = \"/content/drive/MyDrive/MultimodalRewardModel/data\"\n",
        "preference_filepath = f\"{DATA_DIR}/train_preference.jsonl\"\n",
        "with open(preference_filepath, \"w\") as f:\n",
        "    for r in preference_data:\n",
        "        f.write(json.dumps(r) + '\\n')\n",
        "\n",
        "print(f\"\\n✅ High-quality preference dataset created at: {preference_filepath}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O95jfTVv8zL9",
        "outputId": "98421955-3ecd-422a-bfaf-d2d6274acabd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 5 captured images.\n",
            "\n",
            "✅ High-quality preference dataset created at: /content/drive/MyDrive/MultimodalRewardModel/data/train_preference.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Environment Setup & Defining Core Components\n",
        "\n",
        "Before we can train, we need to install the specialized libraries for large-model training and define our core Python classes and functions.\n",
        "\n",
        "\n",
        "**Installs Libraries:** It installs the specific versions of the libraries we need.\n",
        "    * `bitsandbytes`: Crucial for enabling 4-bit quantization, which dramatically reduces memory usage.\n",
        "    * `transformers`, `accelerate`: The core Hugging Face libraries for loading and managing large models.\n"
      ],
      "metadata": {
        "id": "Clfdd7VFb0i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a specific, known-good set of library versions\n",
        "!pip install -q tokenizers bitsandbytes\n",
        "!pip install -U transformers>=4.45.0 accelerate safetensors huggingface_hub\n",
        "!pip install -q torch torchvision Pillow tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci6a_SoNSKZz",
        "outputId": "45e27ce1-6d88-4ff5-d9e0-eab02f58a9ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Training the Multimodal Reward Model\n",
        "\n",
        "This is the heart of our project. In this cell, we load the massive pre-trained LLaVA model, apply our custom modifications, and fine-tune it on our preference dataset to teach it our preferences.\n",
        "\n",
        "**What this code does:**\n",
        "\n",
        "-1.  **Defines the `PreferenceDataset` Class:** This is a standard PyTorch `Dataset` class that knows how to read our `train_preference.jsonl` file and prepare the data (images and text) for the model.\n",
        "\n",
        "0.  **Defines the `reward_model_forward` Function:** This custom function defines the \"brain\" of our reward model. It will be \"monkey-patched\" onto the base LLaVA model to add a final linear layer (`reward_head`) that outputs a single reward score.\n",
        "\n",
        "1.  **Sets Hyperparameters:** At the top, we define key settings for our training run, like the `LEARNING_RATE` and `NUM_EPOCHS`.\n",
        "2.  **Loads Processor & Model:** It loads the LLaVA 1.5 (7B) model. Crucially, it uses two state-of-the-art optimizations:\n",
        "    * **4-bit Quantization:** `BitsAndBytesConfig(load_in_4bit=True)` shrinks the model's size in memory from ~28GB to ~4GB.\n",
        "    * **`device_map=\"auto\"`:** The `accelerate` library intelligently splits the quantized model across the GPU and CPU, ensuring it fits.\n",
        "3.  **Enables Gradient Checkpointing:** `model.gradient_checkpointing_enable()` is another key memory-saving technique that trades a small amount of extra computation time for a massive reduction in memory usage from activations.\n",
        "4.  **Adds the Reward Head:** It dynamically adds our custom, trainable `reward_head` layer to the model and replaces its default `forward` method with our own.\n",
        "5.  **Executes the Manual Training Loop:** Instead of a high-level `Trainer`, we use a manual PyTorch loop for maximum control. For each batch of data, it:\n",
        "    * Calculates the reward score for the `chosen` and `rejected` answers.\n",
        "    * Computes the **Log Sigmoid loss**, which penalizes the model if the rejected score is not lower than the chosen score.\n",
        "    * Uses **Gradient Accumulation** to simulate a larger batch size and stabilize training.\n",
        "    * Updates the model weights using the `AdamW8bit` optimizer.\n",
        "6.  **Saves a Complete Checkpoint:** At the end of each epoch, it saves the base model, the trained `reward_head` weights, and the `processor` (tokenizer + image processor) files to Google Drive, ensuring we have a complete, reloadable artifact."
      ],
      "metadata": {
        "id": "tO4oKwAfb_SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import types\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    LlavaForConditionalGeneration, get_scheduler, BitsAndBytesConfig,\n",
        "    CLIPImageProcessor, LlamaTokenizerFast, LlavaProcessor\n",
        ")\n",
        "import bitsandbytes as bnb\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install -q --upgrade bitsandbytes\n",
        "!pip install -q transformers torch torchvision Pillow tqdm accelerate\n",
        "\n",
        "# (The PreferenceDataset and reward_model_forward functions are unchanged)\n",
        "class PreferenceDataset(Dataset):\n",
        "    def __init__(self, jsonl_file, processor):\n",
        "        self.data = [json.loads(line) for line in open(jsonl_file)]\n",
        "        self.processor = processor\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image = Image.open(item['image']).convert(\"RGB\")\n",
        "        prompt = f\"USER: <image>\\n{item['prompt']}\\nASSISTANT: \"\n",
        "        new_max_length = 2048\n",
        "        inputs_chosen = self.processor(text=prompt + item['chosen'], images=image, return_tensors=\"pt\", padding=\"max_length\", max_length=new_max_length, truncation=True)\n",
        "        inputs_rejected = self.processor(text=prompt + item['rejected'], images=image, return_tensors=\"pt\", padding=\"max_length\", max_length=new_max_length, truncation=True)\n",
        "        return {\n",
        "            \"chosen_pixel_values\": inputs_chosen.pixel_values.squeeze(0), \"chosen_input_ids\": inputs_chosen.input_ids.squeeze(0), \"chosen_attention_mask\": inputs_chosen.attention_mask.squeeze(0),\n",
        "            \"rejected_pixel_values\": inputs_rejected.pixel_values.squeeze(0), \"rejected_input_ids\": inputs_rejected.input_ids.squeeze(0), \"rejected_attention_mask\": inputs_rejected.attention_mask.squeeze(0),\n",
        "        }\n",
        "\n",
        "def reward_model_forward(self, pixel_values, input_ids, attention_mask, **kwargs):\n",
        "    outputs = LlavaForConditionalGeneration.forward(\n",
        "        self, pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True\n",
        "    )\n",
        "    last_hidden_state = outputs.hidden_states[-1][:, -1, :]\n",
        "    last_hidden_state = last_hidden_state.to(self.reward_head.weight.dtype)\n",
        "    reward = self.reward_head(last_hidden_state)\n",
        "    return reward\n",
        "\n",
        "# Main Setup and Training\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/MultimodalRewardModel\"\n",
        "DATA_DIR = f\"{PROJECT_DIR}/data\"\n",
        "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
        "REVISION = \"a272c74\"\n",
        "\n",
        "# Hyperparameter Configuration\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# 1. Load tokenizer and image processor components\n",
        "tokenizer = LlamaTokenizerFast.from_pretrained(MODEL_ID, revision=REVISION)\n",
        "image_processor = CLIPImageProcessor.from_pretrained(MODEL_ID, revision=REVISION)\n",
        "\n",
        "# 2. Combine them using the specific LlavaProcessor class\n",
        "processor = LlavaProcessor(image_processor=image_processor, tokenizer=tokenizer)\n",
        "\n",
        "# =============================================================================\n",
        "# THE FINAL FIX: Manually set the missing attributes on the processor object.\n",
        "# =============================================================================\n",
        "if getattr(processor, 'patch_size', None) is None:\n",
        "    processor.patch_size = 14\n",
        "if getattr(processor, 'num_additional_image_tokens', None) is None:\n",
        "    processor.num_additional_image_tokens = 576\n",
        "print(\"✅ Processor loaded and manually patched successfully.\")\n",
        "# =============================================================================\n",
        "\n",
        "# Configure and load model\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID, revision=REVISION, quantization_config=bnb_config, device_map=\"auto\"\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Modify the model in-place\n",
        "lm_head_device = model.get_output_embeddings().weight.device\n",
        "hidden_size = model.config.text_config.hidden_size\n",
        "model.reward_head = torch.nn.Linear(hidden_size, 1, bias=False).to(lm_head_device).to(torch.float32)\n",
        "model.forward = types.MethodType(reward_model_forward, model)\n",
        "print(f\"Model loaded with 4-bit quantization. Reward head is on device: {model.reward_head.weight.device}\")\n",
        "\n",
        "# Prepare for Training\n",
        "train_dataset = PreferenceDataset(f\"{DATA_DIR}/train_preference.jsonl\", processor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=LEARNING_RATE)\n",
        "gradient_accumulation_steps = 4\n",
        "num_update_steps_per_epoch = len(train_dataloader) // gradient_accumulation_steps\n",
        "if num_update_steps_per_epoch == 0: num_update_steps_per_epoch = 1\n",
        "num_training_steps = NUM_EPOCHS * num_update_steps_per_epoch\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=3, num_training_steps=num_training_steps)\n",
        "progress_bar = tqdm(range(len(train_dataloader) * NUM_EPOCHS))\n",
        "\n",
        "# The Training Loop\n",
        "model.train()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        target_device = model.get_input_embeddings().weight.device\n",
        "        batch = {k: v.to(target_device) for k, v in batch.items()}\n",
        "        rewards_chosen = model(**{k.replace('chosen_',''): v for k, v in batch.items() if 'chosen' in k})\n",
        "        rewards_rejected = model(**{k.replace('rejected_',''): v for k, v in batch.items() if 'rejected' in k})\n",
        "        loss = -torch.nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"WARNING: NaN loss detected at step {i+1} of epoch {epoch+1}. Skipping update.\")\n",
        "            continue\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "        if (i + 1) % gradient_accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        progress_bar.set_description(f\"Epoch {epoch+1}, Loss: {loss.item() * gradient_accumulation_steps:.4f}\")\n",
        "\n",
        "    # --- Correct Saving Logic ---\n",
        "    run_name = f\"lr{LEARNING_RATE}_epochs{NUM_EPOCHS}\"\n",
        "    epoch_dir = f\"{PROJECT_DIR}/reward_model_checkpoint/{run_name}/epoch_{epoch+1}\"\n",
        "    os.makedirs(epoch_dir, exist_ok=True)\n",
        "    print(f\"\\nSaving complete checkpoint for epoch {epoch+1} at {epoch_dir}\")\n",
        "    torch.save(model.reward_head.state_dict(), f\"{epoch_dir}/reward_head.pt\")\n",
        "    model.save_pretrained(epoch_dir)\n",
        "    # Explicitly save the processor's components\n",
        "    tokenizer.save_pretrained(epoch_dir)\n",
        "    image_processor.save_pretrained(epoch_dir)\n",
        "\n",
        "print(\"🎉🎉🎉 Manual training loop complete! 🎉🎉🎉\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "37476513c7f54e9f98a3979dccf77a6b",
            "8d9438c16037497f929b32275ca26b59",
            "4ee301d78f9249d5bcca110c856931d7",
            "ab5d981d8a734636b3c6550bc4480c02",
            "4f94a40875a045a2a16854fc68474c9c",
            "e83f839bd6764f8c8d6dee31f68201f4",
            "a708f9012a8345648846edc09c2ed56a",
            "142ab2c03c384cfb82907c1962e6eebe",
            "d1fb158297f54aa793ce583b0509ef8f",
            "223fe3e1bd7c4a8fb89da54584db4cdf",
            "d0563fbda2a74f0ba2cef104f4dc060c",
            "904b2bf72b254b0d9ff42196967e7d4f",
            "3527c4a2525c46818026f172b42ac722",
            "210f7c0b1977494e86f38ee74b140969",
            "86a5c8e2a67f4a6aa3c67dc07604ea74",
            "dda7450eafa04fe19021587ee24427ac",
            "4d64753a7e654210806d022883c4cb15",
            "7c9df362bfaf41db99ee25220b5bd8fe",
            "d8c59f10d71a4175880bf9bfe21b2e74",
            "19beb63148c443fca20126bd404209f3",
            "b90c79a049e04174813194b513f11440",
            "941abf1e2bb945c787a78acc98e65cb5"
          ]
        },
        "id": "bkwY90T2Ldpp",
        "outputId": "040d8bf4-b9d2-48a2-b0fe-ecc75be8182c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Processor loaded and manually patched successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37476513c7f54e9f98a3979dccf77a6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded with 4-bit quantization. Reward head is on device: cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "904b2bf72b254b0d9ff42196967e7d4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving complete checkpoint for epoch 1 at /content/drive/MyDrive/MultimodalRewardModel/reward_model_checkpoint/lr0.0002_epochs5/epoch_1\n",
            "\n",
            "Saving complete checkpoint for epoch 2 at /content/drive/MyDrive/MultimodalRewardModel/reward_model_checkpoint/lr0.0002_epochs5/epoch_2\n",
            "\n",
            "Saving complete checkpoint for epoch 3 at /content/drive/MyDrive/MultimodalRewardModel/reward_model_checkpoint/lr0.0002_epochs5/epoch_3\n",
            "\n",
            "Saving complete checkpoint for epoch 4 at /content/drive/MyDrive/MultimodalRewardModel/reward_model_checkpoint/lr0.0002_epochs5/epoch_4\n",
            "\n",
            "Saving complete checkpoint for epoch 5 at /content/drive/MyDrive/MultimodalRewardModel/reward_model_checkpoint/lr0.0002_epochs5/epoch_5\n",
            "🎉🎉🎉 Manual training loop complete! 🎉🎉🎉\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Testing and Evaluating Our Reward Model\n",
        "\n",
        "After training is complete, we need to perform inference to verify that our model has learned correctly.\n",
        "\n",
        "**What this code does:**\n",
        "1.  **Configures the Checkpoint:** At the top, you can specify which saved checkpoint you want to load for testing.\n",
        "2.  **Loads the Trained Model:** It loads the base LLaVA model and processor from the specified checkpoint directory.\n",
        "3.  **Attaches the Custom Reward Head:** It re-creates the `reward_head` layer and loads the fine-tuned weights that we saved separately during training.\n",
        "4.  **Runs Inference:** It defines a `get_reward_score` function that takes a new image, prompt, and answer, and returns the model's score. It runs in `torch.no_grad()` mode for efficiency.\n",
        "5.  **Compares Good vs. Bad Answers:** The script runs a test case with a \"good\" answer and a \"bad\" answer. The goal is to see if the model assigns a higher score to the good answer, which proves it has learned our preference. You can easily test with an image from a URL or by uploading your own."
      ],
      "metadata": {
        "id": "m35kvwfncVBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import types\n",
        "from transformers import (\n",
        "    AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        ")\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "import warnings\n",
        "\n",
        "# Suppress unnecessary warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# =============================================================================\n",
        "# --- 1. CONFIGURE WHICH CHECKPOINT TO TEST ---\n",
        "# =============================================================================\n",
        "#\n",
        "# Put the hyperparameters of the run you want to test here.\n",
        "# This must match the folder name of your saved checkpoint.\n",
        "#\n",
        "RUN_LEARNING_RATE = 0.0002\n",
        "RUN_NUM_EPOCHS = 5\n",
        "EPOCH_TO_TEST = 5 # Which epoch from that run to load\n",
        "\n",
        "# --- Automatically construct the path to the checkpoint directory ---\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/MultimodalRewardModel\"\n",
        "RUN_NAME = f\"lr{RUN_LEARNING_RATE}_epochs{NUM_EPOCHS}\"\n",
        "CHECKPOINT_TO_LOAD = f\"{PROJECT_DIR}/reward_model_checkpoint/{RUN_NAME}/epoch_{EPOCH_TO_TEST}\"\n",
        "\n",
        "print(f\"✅ Will attempt to load checkpoint from: {CHECKPOINT_TO_LOAD}\")\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "# --- Define the forward pass function again (needed for monkey-patching) ---\n",
        "def reward_model_forward(self, pixel_values, input_ids, attention_mask, **kwargs):\n",
        "    outputs = LlavaForConditionalGeneration.forward(\n",
        "        self, pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True\n",
        "    )\n",
        "    last_hidden_state = outputs.hidden_states[-1][:, -1, :]\n",
        "    last_hidden_state = last_hidden_state.to(self.reward_head.weight.dtype)\n",
        "    reward = self.reward_head(last_hidden_state)\n",
        "    return reward\n",
        "\n",
        "\n",
        "# --- 2. Load the fine-tuned model and processor from your chosen checkpoint ---\n",
        "print(f\"\\nLoading model and processor from: {CHECKPOINT_TO_LOAD}\")\n",
        "try:\n",
        "    # Load the full processor from the directory\n",
        "  processor = AutoProcessor.from_pretrained(CHECKPOINT_TO_LOAD)\n",
        "\n",
        "    # =============================================================================\n",
        "    # THE FIX IS HERE: Manually set the missing attributes on the loaded processor\n",
        "    # =============================================================================\n",
        "  if getattr(processor, 'patch_size', None) is None:\n",
        "      print(\"Patch size is missing after loading, manually setting to 14.\")\n",
        "      processor.patch_size = 14\n",
        "  if getattr(processor, 'num_additional_image_tokens', None) is None:\n",
        "      print(\"Num additional image tokens is missing, manually setting to 576.\")\n",
        "      processor.num_additional_image_tokens = 576\n",
        "  bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "  model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        CHECKPOINT_TO_LOAD,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "except OSError:\n",
        "    print(f\"❌ ERROR: Checkpoint directory not found at {CHECKPOINT_TO_LOAD}\")\n",
        "    print(\"Please make sure the configuration at the top of the script matches a saved checkpoint.\")\n",
        "    raise\n",
        "\n",
        "# --- 3. Re-create and load your custom reward head ---\n",
        "print(\"Attaching and loading weights for custom reward head...\")\n",
        "hidden_size = model.config.text_config.hidden_size\n",
        "reward_head_path = f\"{CHECKPOINT_TO_LOAD}/reward_head.pt\"\n",
        "\n",
        "if os.path.exists(reward_head_path):\n",
        "    model.reward_head = torch.nn.Linear(hidden_size, 1, bias=False)\n",
        "    model.reward_head.load_state_dict(torch.load(reward_head_path))\n",
        "    model.reward_head.to(model.device)\n",
        "    print(\"✅ Reward head loaded successfully.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"reward_head.pt not found in {CHECKPOINT_TO_LOAD}\")\n",
        "\n",
        "# --- 4. Re-apply the custom forward method and set to eval mode ---\n",
        "model.forward = types.MethodType(reward_model_forward, model)\n",
        "model.eval() # Set the model to evaluation mode (important!)\n",
        "print(\"✅ Model is ready for inference!\")\n",
        "\n",
        "\n",
        "# --- 5. Define an inference function ---\n",
        "def get_reward_score(image_path, prompt_text, answer_text):\n",
        "    \"\"\"\n",
        "    Loads an image and returns a reward score for a given prompt and answer.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        inputs = processor(text=f\"USER: <image>\\n{prompt_text}\\nASSISTANT: {answer_text}\", images=image, return_tensors=\"pt\")\n",
        "        target_device = model.get_input_embeddings().weight.device\n",
        "        inputs = {k: v.to(target_device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            score = model(**inputs)\n",
        "        return score.item()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Image not found at path: {image_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during inference: {e}\")\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# --- 6. RUN YOUR TEST! ---\n",
        "# =============================================================================\n",
        "\n",
        "### --- OPTION A: Test with an Image from a URL ---\n",
        "# By default, this option will run. Change the URL to test different images.\n",
        "\n",
        "# print(\"\\n--- Testing with an image from a URL ---\")\n",
        "# # An example screenshot of a GitHub repository page\n",
        "# image_url = \"https://pageflows.com/media/videos/Booking_a_Room_on_Booking.com.mp4-screenshot-.jpg\"\n",
        "# local_filename = \"test_image_from_url.jpg\"\n",
        "\n",
        "# # Download the image\n",
        "# try:\n",
        "#     response = requests.get(image_url, stream=True)\n",
        "#     response.raise_for_status()\n",
        "#     with open(local_filename, 'wb') as f:\n",
        "#         for chunk in response.iter_content(chunk_size=8192):\n",
        "#             f.write(chunk)\n",
        "#     print(f\"Successfully downloaded image to {local_filename}\")\n",
        "\n",
        "#     # --- Run the test ---\n",
        "#     test_prompt = \"What is this webpage showing?\"\n",
        "#     good_answer = \"This is the page of a Booking.com, showing Stays, Flights, Hotels, Car Rentals.\"\n",
        "#     bad_answer = \"This is a social media profile page with posts and comments.\"\n",
        "\n",
        "#     score_good = get_reward_score(local_filename, test_prompt, good_answer)\n",
        "#     score_bad = get_reward_score(local_filename, test_prompt, bad_answer)\n",
        "\n",
        "#     print(\"\\n--- TEST RESULTS ---\")\n",
        "#     print(f\"Good Answer Score: {score_good:.4f}\")\n",
        "#     print(f\"Bad Answer Score:  {score_bad:.4f}\")\n",
        "\n",
        "#     if score_good is not None and score_bad is not None:\n",
        "#         if score_good > score_bad:\n",
        "#             print(\"\\n✅ Success! The model correctly gave the good answer a higher reward.\")\n",
        "#         else:\n",
        "#             print(\"\\n❌ Failure. The model did not rank the answers as expected.\")\n",
        "\n",
        "# except requests.exceptions.RequestException as e:\n",
        "#     print(f\"Failed to download image from URL: {e}\")\n",
        "\n",
        "\n",
        "### --- OPTION B: Test with an Image from Your Computer ---\n",
        "# To use this, comment out the \"OPTION A\" block above and uncomment this one.\n",
        "\n",
        "from google.colab import files\n",
        "print(\"\\nPlease upload a screenshot to test...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    test_image_path = next(iter(uploaded))\n",
        "    print(f\"\\nUsing uploaded image: {test_image_path}\")\n",
        "\n",
        "    # Define your own prompt and answers for your uploaded image\n",
        "    test_prompt = \"What is the main topic of this webpage?\"\n",
        "    good_answer = \"This is a website displaying products.\"\n",
        "    bad_answer = \"This is a social media site.\"\n",
        "\n",
        "    score_good = get_reward_score(test_image_path, test_prompt, good_answer)\n",
        "    score_bad = get_reward_score(test_image_path, test_prompt, bad_answer)\n",
        "\n",
        "    print(\"\\n--- TEST RESULTS ---\")\n",
        "    print(f\"Good Answer Score: {score_good:.4f}\")\n",
        "    print(f\"Bad Answer Score:  {score_bad:.4f}\")\n",
        "    if score_good is not None and score_bad is not None:\n",
        "        if score_good > score_bad:\n",
        "            print(\"\\n✅ Success! The model correctly gave the good answer a higher reward.\")\n",
        "        else:\n",
        "            print(\"\\n❌ Failure. The model did not rank the answers as expected.\")\n",
        "else:\n",
        "    print(\"No file uploaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "1H8PUFgAIpJZ",
        "outputId": "3a132bc9-8b4f-48b1-901d-24e152f53426"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Will attempt to load checkpoint from: /content/drive/MyDrive/MultimodalRewardModel/reward_model_checkpoint/lr0.0002_epochs5/epoch_5\n",
            "\n",
            "Loading model and processor from: /content/drive/MyDrive/MultimodalRewardModel/reward_model_checkpoint/lr0.0002_epochs5/epoch_5\n",
            "Patch size is missing after loading, manually setting to 14.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/MultimodalRewardModel/reward_model_checkpoint/lr0.0002_epochs5/epoch_5 were not used when initializing LlavaForConditionalGeneration: ['reward_head.weight']\n",
            "- This IS expected if you are initializing LlavaForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LlavaForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attaching and loading weights for custom reward head...\n",
            "✅ Reward head loaded successfully.\n",
            "✅ Model is ready for inference!\n",
            "\n",
            "Please upload a screenshot to test...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-43d7e5e2-8647-431a-8bae-d2eaa1aa3f80\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-43d7e5e2-8647-431a-8bae-d2eaa1aa3f80\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Screenshot 2025-06-17 at 7.54.02 PM.png to Screenshot 2025-06-17 at 7.54.02 PM (3).png\n",
            "\n",
            "Using uploaded image: Screenshot 2025-06-17 at 7.54.02 PM (3).png\n",
            "\n",
            "--- TEST RESULTS ---\n",
            "Good Answer Score: -1.3242\n",
            "Bad Answer Score:  -2.5322\n",
            "\n",
            "✅ Success! The model correctly gave the good answer a higher reward.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "X96t1aQBKJty"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Zu_0U0xYaH_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}